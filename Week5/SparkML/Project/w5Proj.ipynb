{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Используйте как путь куда сохранить модель\n",
    "MODEL_PATH = 'spark_ml_model'\n",
    "\n",
    "\n",
    "def process(spark, train_data, test_data):\n",
    "    #train_data - путь к файлу с данными для обучения модели\n",
    "    #test_data - путь к файлу с данными для оценки качества модели\n",
    "    \n",
    "    #Загрузим датасеты\n",
    "    trainSet = pqt_into_spark(spark, train_data)\n",
    "    testSet = pqt_into_spark(spark, test_data)\n",
    "    \n",
    "    # составим feature vectors (не используя ad_id)\n",
    "    # Переименуем 'ctr' в 'label' - нужно для кросс-валидации\n",
    "    trainSet = trainSet.withColumnRenamed(\"ctr\", \"label\")\n",
    "    testSet = testSet.withColumnRenamed(\"ctr\", \"label\")\n",
    "    features = VectorAssembler(inputCols = trainSet.columns[1:-1], outputCol = 'features')\n",
    "    \n",
    "    # Создадим индексированный вектор для категориальных фич\n",
    "    features_indexing = VectorIndexer(inputCol = 'features', outputCol = 'indexedFeatures', maxCategories = 2)\n",
    "    \n",
    "    # Создадим этап пайплайна для тренировки моделей\n",
    "    lr = LinearRegression(featuresCol = 'features', labelCol = 'label')\n",
    "    (dt, rf, gbt) = [x(featuresCol = 'indexedFeatures', \n",
    "                       labelCol = 'label') \n",
    "                     for x in [DecisionTreeRegressor, RandomForestRegressor, GBTRegressor]]\n",
    "    \n",
    "    # Соберем пайплайны для моделей\n",
    "    pipelines = {\n",
    "    \"pipelineLR\" : Pipeline(stages = [features, lr]),\n",
    "    \"pipelineDT\" : Pipeline(stages = [features, features_indexing, dt]),\n",
    "    \"pipelineRF\" : Pipeline(stages = [features, features_indexing, rf]),\n",
    "    \"pipelineGBT\" : Pipeline(stages = [features, features_indexing, gbt])\n",
    "    }\n",
    "    \n",
    "    # Соберем paramGrids для кросс-валидации\n",
    "    paramGrids = {\n",
    "        \"paramGridLR\" : ParamGridBuilder() \\\n",
    "            .addGrid(lr.maxIter, [10, 20, 40, 80, 150, 300]) \\\n",
    "            .addGrid(lr.regParam, [0.1, 0.2, 0.4, 0.6, 0.8, 0.9]) \\\n",
    "            .addGrid(lr.elasticNetParam, [0.5, 0.6, 0.7, 0.8, 0.9])\\\n",
    "            .build(),\n",
    "        \"paramGridDT\" : ParamGridBuilder() \\\n",
    "            .addGrid(dt.maxBins, [24, 28, 32, 36, 40]) \\\n",
    "            .addGrid(dt.maxDepth, [3, 4, 5, 6, 7]) \\\n",
    "            .build(),\n",
    "        \"paramGridRF\" : ParamGridBuilder() \\\n",
    "            .addGrid(rf.numTrees, [10, 15, 20, 25, 30]) \\\n",
    "            .addGrid(rf.maxBins, [24, 28, 32, 36, 40]) \\\n",
    "            .addGrid(rf.maxDepth, [3, 4, 5, 6, 7]) \\\n",
    "            .build(),\n",
    "        \"paramGridGBT\" : ParamGridBuilder() \\\n",
    "            .addGrid(gbt.maxDepth, [3, 4, 5, 6, 7]) \\\n",
    "            .addGrid(gbt.maxBins, [24, 28, 32, 36, 40]) \\\n",
    "            .addGrid(gbt.stepSize, [0.05, 0.1, 0.2]) \\\n",
    "            .build()\n",
    "    }\n",
    "    \n",
    "    # подготовим разные кросс-валидированные модели -- ОЧЕНЬ долгий этап\n",
    "    cv = {modeltype : CrossValidator(estimator=pipelines[\"pipeline\" + modeltype],\n",
    "                          estimatorParamMaps=paramGrids[\"paramGrid\" + modeltype],\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=3).fit(trainSet)\n",
    "          for modeltype in [\"LR\", \"DT\", \"RF\", \"GBT\"]}\n",
    "    \n",
    "    # подготовим предсказания с использованием этих кросс-валидированных моделей, посчитаем RMSE и выберем лучшую\n",
    "    cv_predictions = {modeltype : cv[modeltype].transform(testSet) for modeltype in cv}\n",
    "    evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    evaluation = {modeltype : evaluator.evaluate(cv_predictions[modeltype]) for modeltype in cv_predictions}\n",
    "\n",
    "    # Выберем модель с наименьшей RMSE и сохраним в указанную директорию\n",
    "    bestModel = cv[min(evaluation.items())[0]]\n",
    "    bestModel.save(MODEL_PATH)\n",
    "    \n",
    "def pqt_into_spark(spark, input_file):\n",
    "    # Эта функция прицельно загружает паркет-файл в спарк\n",
    "    sparkdata = spark.read.parquet(input_file)\n",
    "    return sparkdata\n",
    "\n",
    "def main(argv):\n",
    "    train_data = argv[0]\n",
    "    print(\"Input path to train data: \" + train_data)\n",
    "    test_data = argv[1]\n",
    "    print(\"Input path to test data: \" + test_data)\n",
    "    spark = _spark_session()\n",
    "    process(spark, train_data, test_data)\n",
    "\n",
    "\n",
    "def _spark_session():\n",
    "    return SparkSession.builder.appName('PySparkMLFitJob').getOrCreate()\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    arg = sys.argv[1:]\n",
    "#    if len(arg) != 2:\n",
    "#        sys.exit(\"Train and test data are require.\")\n",
    "#    else:\n",
    "#        main(arg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = _spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pqt_into_spark(spark, input_file):\n",
    "    # Эта функция прицельно загружает паркет-файл в спарк\n",
    "    sparkdata = spark.read.parquet(input_file)\n",
    "    return sparkdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = \"./train.parquet\"\n",
    "test_data = \"./test.parquet\"\n",
    "\n",
    "trainSet = pqt_into_spark(spark, train_data)\n",
    "testSet = pqt_into_spark(spark, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+---------+------+------+----------------+---------+-----------------+\n",
      "|ad_id|target_audience_count|has_video|is_cpm|is_cpc|         ad_cost|day_count|              ctr|\n",
      "+-----+---------------------+---------+------+------+----------------+---------+-----------------+\n",
      "|    1|     10707.2440058622|        1|     1|     0|201.829292651124|       15|0.431740082807281|\n",
      "|    5|     10643.3872649482|        1|     1|     0|192.577221699704|       15|0.809264519216201|\n",
      "|    6|     11418.7085911347|        1|     1|     0|204.104562956739|       11|0.909738306804039|\n",
      "|    7|     10109.3278687796|        1|     1|     0|194.255798599684|       12|0.941221039774456|\n",
      "|    8|     10665.1119991977|        1|     1|     0|202.658042557742|       14|0.986790019690954|\n",
      "|    9|     10888.7521785156|        1|     1|     0|197.085338772736|       15|0.995306486518015|\n",
      "|   11|     9637.20484730933|        1|     1|     0|192.092306095236|       18| 1.02222752080496|\n",
      "|   12|     9886.86231469735|        1|     1|     0|199.987605376721|       15| 1.02822730862374|\n",
      "|   14|     10247.5276105797|        1|     1|     0|197.731146806599|       13|  1.0794676552788|\n",
      "|   17|     9045.39924498352|        1|     1|     0|198.584786530274|       14| 1.19230724476475|\n",
      "|   19|     9272.23637258281|        1|     1|     0|195.191310260202|       13| 1.19683538489291|\n",
      "|   21|     9568.62828947957|        1|     1|     0|199.557502134239|       11| 1.23608059326114|\n",
      "|   23|     8495.13059456543|        1|     1|     0|199.408031330258|       15| 1.25409556199282|\n",
      "|   24|     8891.97983774145|        1|     1|     0|199.158928072324|       15| 1.25439490657975|\n",
      "|   25|     9001.24662789885|        1|     1|     0|201.935855773451|       16| 1.27404754439317|\n",
      "|   26|     9715.31326725688|        1|     1|     0|195.074715187183|       16| 1.29167178084748|\n",
      "|   27|     9147.65018866017|        1|     1|     0|196.168514471689|       14| 1.30790620835937|\n",
      "|   28|     9794.85432684069|        1|     1|     0|205.079136092444|       16| 1.32626300321519|\n",
      "|   29|     9280.78868307884|        1|     1|     0|202.117426517002|       15|  1.3317748526955|\n",
      "|   30|     10588.5616689154|        1|     1|     0|198.140115906802|       10| 1.34577751718022|\n",
      "+-----+---------------------+---------+------+------+----------------+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainSet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    trainSet = trainSet.withColumnRenamed(\"ctr\", \"label\")\n",
    "    testSet = testSet.withColumnRenamed(\"ctr\", \"label\")\n",
    "    features = VectorAssembler(inputCols = trainSet.columns[1:-1], outputCol = 'features')\n",
    "    features_indexing = VectorIndexer(inputCol = 'features', outputCol = 'indexedFeatures', maxCategories = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol = 'features', labelCol = 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    (dt, rf, gbt) = [x(featuresCol = 'indexedFeatures', \n",
    "                       labelCol = 'label') \n",
    "                     for x in [DecisionTreeRegressor, RandomForestRegressor, GBTRegressor]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    pipelines = {\n",
    "    \"pipelineLR\" : Pipeline(stages = [features, lr]),\n",
    "    \"pipelineDT\" : Pipeline(stages = [features, features_indexing, dt]),\n",
    "    \"pipelineRF\" : Pipeline(stages = [features, features_indexing, rf]),\n",
    "    \"pipelineGBT\" : Pipeline(stages = [features, features_indexing, gbt])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    paramGrids = {\n",
    "        \"paramGridLR\" : ParamGridBuilder() \\\n",
    "            .addGrid(lr.maxIter, [10, 20, 40, 80, 150, 300]) \\\n",
    "            .addGrid(lr.regParam, [0.1, 0.2, 0.4, 0.6, 0.8, 0.9]) \\\n",
    "            .addGrid(lr.elasticNetParam, [0.5, 0.6, 0.7, 0.8, 0.9])\\\n",
    "            .build(),\n",
    "        \"paramGridDT\" : ParamGridBuilder() \\\n",
    "            .addGrid(dt.maxBins, [24, 28, 32, 36, 40]) \\\n",
    "            .addGrid(dt.maxDepth, [3, 4, 5, 6, 7]) \\\n",
    "            .build(),\n",
    "        \"paramGridRF\" : ParamGridBuilder() \\\n",
    "            .addGrid(rf.numTrees, [10, 15, 20, 25, 30]) \\\n",
    "            .addGrid(rf.maxBins, [24, 28, 32, 36, 40]) \\\n",
    "            .addGrid(rf.maxDepth, [3, 4, 5, 6, 7]) \\\n",
    "            .build(),\n",
    "        \"paramGridGBT\" : ParamGridBuilder() \\\n",
    "            .addGrid(gbt.maxDepth, [3, 4, 5, 6, 7]) \\\n",
    "            .addGrid(gbt.maxBins, [24, 28, 32, 36, 40]) \\\n",
    "            .addGrid(gbt.stepSize, [0.05, 0.1, 0.2]) \\\n",
    "            .build()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2ea5266dfe9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m cv = {modeltype : CrossValidator(estimator=pipelines[\"pipeline\" + modeltype],\n\u001b[0m\u001b[1;32m      2\u001b[0m                       \u001b[0mestimatorParamMaps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparamGrids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"paramGrid\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodeltype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                       \u001b[0mevaluator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRegressionEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                       numFolds=3, parallelism=5).fit(trainSet)\n\u001b[1;32m      5\u001b[0m       for modeltype in [\"LR\", \"DT\", \"RF\", \"GBT\"]}\n",
      "\u001b[0;32m<ipython-input-14-2ea5266dfe9f>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m cv = {modeltype : CrossValidator(estimator=pipelines[\"pipeline\" + modeltype],\n\u001b[0m\u001b[1;32m      2\u001b[0m                       \u001b[0mestimatorParamMaps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparamGrids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"paramGrid\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodeltype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                       \u001b[0mevaluator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRegressionEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                       numFolds=3, parallelism=5).fit(trainSet)\n\u001b[1;32m      5\u001b[0m       for modeltype in [\"LR\", \"DT\", \"RF\", \"GBT\"]}\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    856\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    cv = {modeltype : CrossValidator(estimator=pipelines[\"pipeline\" + modeltype],\n",
    "                          estimatorParamMaps=paramGrids[\"paramGrid\" + modeltype],\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=3, parallelism=5).fit(trainSet)\n",
    "          for modeltype in [\"LR\", \"DT\", \"RF\", \"GBT\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LR': CrossValidatorModel_42dc5693f990,\n",
       " 'DT': CrossValidatorModel_4ee4eafea9b2,\n",
       " 'RF': CrossValidatorModel_3ccdc5a34a8d,\n",
       " 'GBT': CrossValidatorModel_abe291a4cd13}"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predictions = {modeltype : cv[modeltype].transform(testSet) for modeltype in cv}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = {modeltype : evaluator.evaluate(cv_predictions[modeltype]) for modeltype in cv_predictions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = cv[min(evaluation.items())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.tuning.CrossValidatorModel"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bestModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "\n",
    "# Используйте как путь откуда загрузить модель\n",
    "MODEL_PATH = 'spark_ml_model'\n",
    "\n",
    "\n",
    "def process(spark, input_file, output_file):\n",
    "    #input_file - путь к файлу с данными для которых нужно предсказать ctr\n",
    "    #output_file - путь по которому нужно сохранить файл с результатами [ads_id, prediction]\n",
    "    \n",
    "    bestModel = CrossValidatorModel.load(MODEL_PATH)\n",
    "    predictionData = pqt_into_spark(spark, input_file)\n",
    "    \n",
    "    # Применим нашу модель\n",
    "    predictedValues = bestModel.transform(predictionData).select(['ad_id', 'prediction'])\n",
    "    \n",
    "    # Запишем предсказание в файл\n",
    "    predictedValues.coalesce(1).write.csv(output_file)\n",
    "    \n",
    "def pqt_into_spark(spark, input_file):\n",
    "    # Эта функция прицельно загружает паркет-файл в спарк\n",
    "    sparkdata = spark.read.parquet(input_file)\n",
    "    return sparkdata\n",
    "\n",
    "def main(argv):\n",
    "    input_path = argv[0]\n",
    "    print(\"Input path to file: \" + input_path)\n",
    "    output_file = argv[1]\n",
    "    print(\"Output path to file: \" + output_file)\n",
    "    spark = _spark_session()\n",
    "    process(spark, input_path, output_file)\n",
    "\n",
    "\n",
    "def _spark_session():\n",
    "    return SparkSession.builder.appName('PySparkMLPredict').getOrCreate()\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    arg = sys.argv[1:]\n",
    "#    if len(arg) != 2:\n",
    "#        sys.exit(\"Input and Target path are require.\")\n",
    "#    else:\n",
    "#        main(arg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.tuning import CrossValidatorModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'spark_ml_model'\n",
    "input_file = \"./test.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = _spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = CrossValidatorModel.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pqt_into_spark(spark, input_file):\n",
    "    # Эта функция прицельно загружает паркет-файл в спарк\n",
    "    sparkdata = spark.read.parquet(input_file)\n",
    "    return sparkdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictionData = pqt_into_spark(spark, input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " predictedValues = bestModel.transform(predictionData).select(['ad_id', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ad_id=199909, prediction=7.137802258670191),\n",
       " Row(ad_id=199915, prediction=7.137802258670191),\n",
       " Row(ad_id=199919, prediction=7.137802258670191),\n",
       " Row(ad_id=199921, prediction=7.165715294275506),\n",
       " Row(ad_id=199922, prediction=7.165715294275506),\n",
       " Row(ad_id=199926, prediction=7.165715294275506),\n",
       " Row(ad_id=199927, prediction=7.165715294275506),\n",
       " Row(ad_id=199936, prediction=7.071821188068931),\n",
       " Row(ad_id=199941, prediction=7.071821188068931),\n",
       " Row(ad_id=199943, prediction=7.137802258670191),\n",
       " Row(ad_id=199946, prediction=7.137802258670191),\n",
       " Row(ad_id=199953, prediction=7.165715294275506),\n",
       " Row(ad_id=199957, prediction=7.137802258670191),\n",
       " Row(ad_id=199960, prediction=7.165715294275506),\n",
       " Row(ad_id=199961, prediction=7.165715294275506),\n",
       " Row(ad_id=199962, prediction=7.165715294275506),\n",
       " Row(ad_id=199979, prediction=7.137802258670191),\n",
       " Row(ad_id=199982, prediction=7.195304985144779),\n",
       " Row(ad_id=199987, prediction=7.165715294275506),\n",
       " Row(ad_id=199997, prediction=7.165715294275506)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedValues.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ad_id=199909, target_audience_count=2726.00483717456, has_video=1, is_cpm=1, is_cpc=0, ad_cost=202.85202451412, day_count=15, ctr=8.27266867036885),\n",
       " Row(ad_id=199915, target_audience_count=3249.00937186662, has_video=1, is_cpm=1, is_cpc=0, ad_cost=203.086682185664, day_count=15, ctr=8.29511095173232),\n",
       " Row(ad_id=199919, target_audience_count=2168.74700461789, has_video=1, is_cpm=1, is_cpc=0, ad_cost=202.399669590209, day_count=15, ctr=8.30892779651573),\n",
       " Row(ad_id=199921, target_audience_count=2232.58001844152, has_video=1, is_cpm=1, is_cpc=0, ad_cost=198.771291912571, day_count=15, ctr=8.3125568979919),\n",
       " Row(ad_id=199922, target_audience_count=2905.90745280996, has_video=1, is_cpm=1, is_cpc=0, ad_cost=200.430681059033, day_count=18, ctr=8.31504202384652),\n",
       " Row(ad_id=199926, target_audience_count=1504.33904732107, has_video=1, is_cpm=1, is_cpc=0, ad_cost=199.565148016933, day_count=17, ctr=8.33828596983834),\n",
       " Row(ad_id=199927, target_audience_count=2434.71225604191, has_video=1, is_cpm=1, is_cpc=0, ad_cost=198.287384658803, day_count=17, ctr=8.34112779042452),\n",
       " Row(ad_id=199936, target_audience_count=2329.66552285341, has_video=1, is_cpm=1, is_cpc=0, ad_cost=195.106918014661, day_count=15, ctr=8.39961000311375),\n",
       " Row(ad_id=199941, target_audience_count=2646.55479819192, has_video=1, is_cpm=1, is_cpc=0, ad_cost=190.783921111751, day_count=16, ctr=8.40182000740536),\n",
       " Row(ad_id=199943, target_audience_count=3396.04697137784, has_video=1, is_cpm=1, is_cpc=0, ad_cost=204.167038939073, day_count=16, ctr=8.41435401556301),\n",
       " Row(ad_id=199946, target_audience_count=3349.38508564187, has_video=1, is_cpm=1, is_cpc=0, ad_cost=202.615114871534, day_count=16, ctr=8.4274993783567),\n",
       " Row(ad_id=199953, target_audience_count=1812.69280728378, has_video=1, is_cpm=1, is_cpc=0, ad_cost=199.070111575677, day_count=15, ctr=8.47125106591689),\n",
       " Row(ad_id=199957, target_audience_count=2223.91033892607, has_video=1, is_cpm=1, is_cpc=0, ad_cost=204.465147174538, day_count=17, ctr=8.48728462210874),\n",
       " Row(ad_id=199960, target_audience_count=2600.28913799782, has_video=1, is_cpm=1, is_cpc=0, ad_cost=202.21094680142, day_count=14, ctr=8.52346762345823),\n",
       " Row(ad_id=199961, target_audience_count=2455.27594093341, has_video=1, is_cpm=1, is_cpc=0, ad_cost=197.623981196268, day_count=15, ctr=8.52528908888092),\n",
       " Row(ad_id=199962, target_audience_count=2860.49308600368, has_video=1, is_cpm=1, is_cpc=0, ad_cost=199.49765561926, day_count=16, ctr=8.55644406318661),\n",
       " Row(ad_id=199979, target_audience_count=1940.87204893734, has_video=1, is_cpm=1, is_cpc=0, ad_cost=204.164527037897, day_count=12, ctr=8.70226285159093),\n",
       " Row(ad_id=199982, target_audience_count=1140.59087292456, has_video=1, is_cpm=1, is_cpc=0, ad_cost=195.476056731549, day_count=12, ctr=8.72123797969316),\n",
       " Row(ad_id=199987, target_audience_count=2140.75981131993, has_video=1, is_cpm=1, is_cpc=0, ad_cost=200.027992696357, day_count=15, ctr=8.81159852269921),\n",
       " Row(ad_id=199997, target_audience_count=1892.00955700504, has_video=1, is_cpm=1, is_cpc=0, ad_cost=199.955982551855, day_count=15, ctr=9.18581013161957)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionData.tail(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
