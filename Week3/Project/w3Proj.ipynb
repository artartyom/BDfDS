{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \"__main__\":\\n    arg = sys.argv[1:]\\n    if len(arg) != 2:\\n        sys.exit(\"Input and Target path are require.\")\\n    else:\\n        main(arg)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, datediff\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def process(spark, input_file, target_path):\n",
    "    df = pqt_into_spark(spark, input_file)\n",
    "    # Уберем то, что вообще не пригодится\n",
    "    df = df.drop(\"time\", \"platform\", \"client_union_id\", \"compaign_union_id\")\n",
    "    \n",
    "    # Создадим вспомогательные таблицы\n",
    "    suppTables = create_supp_tables(spark, df)\n",
    "    \n",
    "    # Cоберем вспомогательные таблицы в одну\n",
    "    finalTable = suppTables[0] \\\n",
    "    .join(suppTables[1], on = \"ad_id\") \\\n",
    "    .join(suppTables[2], on = \"ad_id\") \\\n",
    "    .join(suppTables[3], on = \"ad_id\")\n",
    "    \n",
    "    # Разделим на train и test сеты\n",
    "    splitData = finalTable.randomSplit([0.75, 0.25])\n",
    "    \n",
    "    # Запишем данные на диск\n",
    "    write_split_data(splitData, target_path)\n",
    "\n",
    "def cnt_conditional(condition):\n",
    "    # для подсчета числа значений, соответствующих заданному условию\n",
    "    return F.sum(F.when(condition, 1).otherwise(0))\n",
    "    \n",
    "def pqt_into_spark(spark, input_file):\n",
    "    # Эта функция прицельно загружает паркет-файл в спарк\n",
    "    sparkdata = spark.read.parquet(input_file)\n",
    "    return sparkdata\n",
    "\n",
    "def create_supp_tables(spark, df):\n",
    "    # Создает вспомогательные таблицы, в которых рассчитаны отдельные группировочные статистики\n",
    "    # Подготовим view для SQL-запросов\n",
    "    df.createOrReplaceTempView(\"sqlDF\")\n",
    "    \n",
    "    # Посчитаем количество дней, которые отображалось каждое объявление\n",
    "    uniqueDays = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT ad_id, count(ad_id) as day_count\n",
    "    FROM (\n",
    "        SELECT ad_id, date\n",
    "        FROM sqlDF\n",
    "        GROUP BY ad_id, date)\n",
    "    GROUP BY ad_id\n",
    "    \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Посчитаем CTR.\n",
    "    \n",
    "    CTR = df \\\n",
    "    .withColumn(\"eventView\", col(\"event\") == \"view\") \\\n",
    "    .withColumn(\"eventClick\", col(\"event\") == \"click\") \\\n",
    "    .groupBy(\"ad_id\") \\\n",
    "    .agg(\n",
    "    cnt_conditional(col(\"eventView\")).alias(\"cnt_views\"),\n",
    "    cnt_conditional(col(\"eventClick\")).alias(\"cnt_clicks\")) \\\n",
    "    .withColumn(\"CTR\", col(\"cnt_clicks\") / col(\"cnt_views\")) \\\n",
    "    .select(\"ad_id\", \"CTR\")\n",
    "    \n",
    "    # Создадим dummy variables для CPC и CPM\n",
    "    \n",
    "    CPC_CPM = df \\\n",
    "    .select(\"ad_id\", \"ad_cost_type\") \\\n",
    "    .groupBy(\"ad_id\") \\\n",
    "    .agg(F.first(F.when(col(\"ad_cost_type\") == \"CPM\", 1).otherwise(0)).alias(\"CPM\"),\n",
    "         F.first(F.when(col(\"ad_cost_type\") == \"CPC\", 1).otherwise(0)).alias(\"CPC\")\n",
    "    )\n",
    "    \n",
    "    # Соберем остальные характеристики каждого объявления (уникальные)\n",
    "    \n",
    "    basicValues = df.select(\"ad_id\", \"target_audience_count\", \"has_video\").distinct()\n",
    "    \n",
    "    return (uniqueDays, CTR, CPC_CPM, basicValues)\n",
    "\n",
    "def write_split_data(splitData, path_out)\n",
    "    # создаем zip с названием датасета и его содержимым\n",
    "    zipdata = zip((\"train\", \"test\"), split)\n",
    "    \n",
    "    for i in zipdata:\n",
    "        # создадим название пути для данных\n",
    "        writepath = path_out + \"/\" + i[0]\n",
    "        # Пишем датафрейм на диск как parquet-файл\n",
    "        i[1].coalesce(1).write.parquet(writepath)\n",
    "\n",
    "def main(argv):\n",
    "    input_path = argv[0]\n",
    "    print(\"Input path to file: \" + input_path)\n",
    "    target_path = argv[1]\n",
    "    print(\"Target path: \" + target_path)\n",
    "    spark = _spark_session()\n",
    "    process(spark, input_path, target_path)\n",
    "\n",
    "\n",
    "def _spark_session():\n",
    "    return SparkSession.builder.appName('PySparkJob').getOrCreate()\n",
    "\n",
    "\n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "    arg = sys.argv[1:]\n",
    "    if len(arg) != 2:\n",
    "        sys.exit(\"Input and Target path are require.\")\n",
    "    else:\n",
    "        main(arg)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('PySparkJob').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'event',\n",
       " 'ad_id',\n",
       " 'ad_cost_type',\n",
       " 'ad_cost',\n",
       " 'has_video',\n",
       " 'target_audience_count']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.parquet(\"./clickstream.parquet\")\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_supp_tables(spark, df):\n",
    "    \n",
    "    # Подготовим view для SQL-запросов\n",
    "    df.createOrReplaceTempView(\"sqlDF\")\n",
    "    \n",
    "    # Посчитаем количество дней, которые отображалось каждое объявление\n",
    "    uniqueDays = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT ad_id, count(ad_id) as day_count\n",
    "    FROM (\n",
    "        SELECT ad_id, date\n",
    "        FROM sqlDF\n",
    "        GROUP BY ad_id, date)\n",
    "    GROUP BY ad_id\n",
    "    \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Посчитаем CTR.\n",
    "    \n",
    "    CTR = df \\\n",
    "    .withColumn(\"eventView\", col(\"event\") == \"view\") \\\n",
    "    .withColumn(\"eventClick\", col(\"event\") == \"click\") \\\n",
    "    .groupBy(\"ad_id\") \\\n",
    "    .agg(\n",
    "    cnt_conditional(col(\"eventView\")).alias(\"cnt_views\"),\n",
    "    cnt_conditional(col(\"eventClick\")).alias(\"cnt_clicks\")) \\\n",
    "    .withColumn(\"CTR\", col(\"cnt_clicks\") / col(\"cnt_views\")) \\\n",
    "    .select(\"ad_id\", \"CTR\")\n",
    "    \n",
    "    # Создадим dummy variables для CPC и CPM\n",
    "    \n",
    "    CPC_CPM = df \\\n",
    "    .select(\"ad_id\", \"ad_cost_type\") \\\n",
    "    .groupBy(\"ad_id\") \\\n",
    "    .agg(F.first(F.when(col(\"ad_cost_type\") == \"CPM\", 1).otherwise(0)).alias(\"CPM\"),\n",
    "         F.first(F.when(col(\"ad_cost_type\") == \"CPC\", 1).otherwise(0)).alias(\"CPC\")\n",
    "    )\n",
    "    \n",
    "    # Соберем остальные характеристики каждого объявления (уникальные)\n",
    "    \n",
    "    basicValues = df.select(\"ad_id\", \"target_audience_count\", \"has_video\").distinct()\n",
    "    \n",
    "    return (basicValues, CPC_CPM, uniqueDays, CTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppTables = create_supp_tables(spark, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "    finalTable = suppTables[0] \\\n",
    "    .join(suppTables[1], on = \"ad_id\") \\\n",
    "    .join(suppTables[2], on = \"ad_id\") \\\n",
    "    .join(suppTables[3], on = \"ad_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = finalTable.randomSplit([0.75, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipdata = zip((\"train\", \"test\"), split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = \"tet\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
